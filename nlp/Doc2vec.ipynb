{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Doc2vec.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOxG25VQHSOKmaR4D4PnjLF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 라이브러리 설치"],"metadata":{"id":"sHsiRHqUoVTt"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"ooQoC8x_Owfp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","apt-get update\n","apt-get install g++ openjdk-8-jdk python-dev python3-dev\n","pip3 install JPype1\n","pip3 install konlpy\n","\n","%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","\n","%%bash\n","bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","pip3 install /tmp/mecab-python-0.996"],"metadata":{"id":"Bn0XQKe8OyHY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 준비"],"metadata":{"id":"K3JNEDjmoeTW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLs1y8GZOfhH"},"outputs":[],"source":["# 구글드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# 데이터셋 불러오기\n","import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/데캡디/review_link10.csv')"],"metadata":{"id":"328_yib43cGb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 전처리"],"metadata":{"id":"Ia3y7YsPoimB"}},{"cell_type":"code","source":["# 해당 문장 제거\n","string =  '※ 해당 리뷰는 원칙적으로 기본 상품이 동일한 단품 사용 후 작성된 것이며,개별 상품에 따라 용량 내지 일부 구성(1+1, 기획상품 등)이 상이할 수 있음을 안내드립니다.'\n","\n","review_clean = []\n","for i in range(len(df)):\n","    temp = df['리뷰'].iloc[i]\n","    review_clean.append(temp.replace(string, \"\"))\n","\n","df['리뷰'] = review_clean\n","\n","df['리뷰'] = df['리뷰'].str.replace('\\n',' ')\n","df['리뷰'] = df['리뷰'].str.replace('  ',' ')\n","\n","target_string = ['피부타입', '복합성에 좋아요', '피부고민', '진정에 좋아요', '자극도', '자극없이 순해요']\n","df = df[~df['리뷰'].map(lambda x: all(string in x for string in target_string))]"],"metadata":{"id":"Up6ws-vH3wY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","import konlpy\n","from konlpy.utils import pprint\n","from konlpy.tag import Mecab\n","\n","from tqdm import tqdm, tqdm_notebook"],"metadata":{"id":"ct-ZwyaK37ip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","import re\n","# 기호, 숫자, 영어 등은 다 제외하고 한글만 남김\n","df['리뷰'] = [re.sub('[0-9]+', '', str(e)) for e in df['리뷰']]\n","df['리뷰'] = df['리뷰'].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)\n","\n","df.head()\n","\n","\n","# 형태소 분석 # 명사 추출\n","\n","def clean_text(text):\n","    text = text.replace(\".\", \" \").strip()\n","    text = text.replace(\".\", \" \").strip()\n","    pattern = '[^ ㄱ-ㅣ가-힣|0-9|a-zA-Z]+'\n","    text = re.sub(pattern=pattern, repl=\"\", string=text)\n","    return text\n","\n","def get_nouns(tokenizer, sentence):\n","    tagged = tokenizer.pos(sentence)\n","    nouns = [s for s, t in tagged if t in ['SL', 'NNG', 'NNP'] and len(s) > 1]\n","    return nouns\n","\n","def tokenize(df):\n","    tokenizer = Mecab()\n","    processed_data = []\n","    for sent in tqdm(df['리뷰']):\n","        sentence = clean_text(sent.replace('\\n', \"\").strip())\n","        processed_data.append(get_nouns(tokenizer, sentence))\n","    return processed_data\n","'''"],"metadata":{"id":"EjYAvyEF4noV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"],"metadata":{"id":"LvyoeBuC-2Kd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pykospacing import Spacing\n","spacing = Spacing()\n","kospacing_sent = spacing(\"김형호영화시장분석가는'1987'의네이버영화정보네티즌10점평에서언급된단어들을지난해12월27일부터올해1월10일까지통계프로그램R과KoNLP패키지로텍스트마이닝하여분석했다.\")\n","print(kospacing_sent)"],"metadata":{"id":"jZ6GTPvF-5KI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/ssut/py-hanspell.git"],"metadata":{"id":"G1OLmRC8Ccc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from hanspell import spell_checker\n","\n","sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n","spelled_sent = spell_checker.check(sent)\n","\n","hanspell_sent = spelled_sent.checked\n","print(hanspell_sent)"],"metadata":{"id":"G5ROmAdlCdh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spelled_sent = spell_checker.check(\"김형호영화시장분석가는'1987'의네이버영화정보네티즌10점평에서언급된단어들을지난해12월27일부터올해1월10일까지통계프로그램R과KoNLP패키지로텍스트마이닝하여분석했다.\")\n","\n","hanspell_sent = spelled_sent.checked\n","print(hanspell_sent)\n","print(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과"],"metadata":{"id":"3YhN-P2sCdqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_product_name = df['상품명'].unique()\n","df = df[df['상품명'] == df_product_name[0]]\n","df"],"metadata":{"id":"Oj6Xn7NiCdtf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 형태소 분석"],"metadata":{"id":"3xFhazW1pNCl"}},{"cell_type":"code","source":["# 형태소 분석 # 명사 추출\n","import re\n","from pykospacing import Spacing\n","from hanspell import spell_checker\n","spacing = Spacing()\n","def clean_text(text):\n","    text = text.replace(\".\", \" \").strip()\n","    text = text.replace(\".\", \" \").strip()\n","    pattern = '[^ ㄱ-ㅣ가-힣|0-9|a-zA-Z]+'\n","    text = re.sub(pattern=pattern, repl=\"\", string=text)\n","    return text\n","\n","def get_nouns(tokenizer, sentence):\n","    tagged = tokenizer.pos(sentence)\n","    nouns = [s for s, t in tagged if t in ['SL', 'NNG', 'NNP'] and len(s) > 1]\n","    return nouns\n","\n","def tokenize(df):\n","    tokenizer = Mecab()\n","    processed_data = []\n","    for sent in tqdm(df['리뷰']):\n","        sentence = clean_text(sent.replace('\\n', \"\").strip())\n","        #kospacing_sent = spacing(sentence)\n","        spelled_sent = spell_checker.check(sentence)\n","        hanspell_sent = spelled_sent.checked\n","        processed_data.append(get_nouns(tokenizer, hanspell_sent))\n","    return processed_data"],"metadata":{"id":"DaRX7dVi9lXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_data = tokenize(df)"],"metadata":{"id":"-VQpXSof9rxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open('/content/drive/MyDrive/데캡디/data_0.pickle', 'wb') as f:\n","    pickle.dump(processed_data, f, pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"ktZveuEoGovl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","with open('/content/drive/MyDrive/데캡디/data_0.pickle', 'rb') as f:\n","    data = pickle.load(f)"],"metadata":{"id":"vp3jcRcFGo6v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['리뷰'].iloc[2]"],"metadata":{"id":"gIwOMpSFWRLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_data[2]"],"metadata":{"id":"R-SiJDXwzrJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"pY4D2Gay9GpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = []\n","for i in data:\n","    for j in i:\n","        vocab.append(j) "],"metadata":{"id":"HgOZ5sxD8U8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab"],"metadata":{"id":"M7CMnQGZ9U85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_set = set(vocab)\n","my_list = list(my_set)\n","my_list"],"metadata":{"id":"LxtKscKB87Xs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Wv2xT-xi9dCX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(processed_data)"],"metadata":{"id":"0CrtKaF0-tC-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Mecab\n","mecab = Mecab()\n"],"metadata":{"id":"eQs7METg8HPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","noun_words = []\n","verb_words = []\n","for i in range(len(temp)):\n","  for word,pos in mecab.pos(temp[i]):\n","    if pos[:1]=='N':\n","      noun_words.append(word)\n","    elif pos[:1]=='V':\n","      verb_words.append(word)\n","'''"],"metadata":{"id":"TPusShAh7nz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","verb_words = {}\n","for i in range(len(temp)):\n","  for word,pos in mecab.pos(temp[i]):\n","    if pos[:1]=='V':\n","      verb_words[pos] = []\n","for i in range(len(temp)):\n","  for word,pos in mecab.pos(temp[i]):\n","    if pos[:1]=='V':\n","      verb_words[pos].append(word)\n","'''"],"metadata":{"id":"G9VgWJDF8S5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for pos in verb_words.keys():\n","  verb_words[pos] = list(set(verb_words[pos]))"],"metadata":{"id":"xjcDAUd78UQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = []\n","for i in range(len(df)):\n","  result.append(mecab.nouns(df.iloc[i,0]))"],"metadata":{"id":"KOWgMw2z8UTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9YK1Am-D8UWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.test.utils import common_texts\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument"],"metadata":{"id":"75VbEGAT5kwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_data"],"metadata":{"id":"iOG7_aDj9y5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words = \"올리브 전체 사용 생각 제품 추가 정도\""],"metadata":{"id":"MtkAY81M0Yqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_texts_and_tags = [\n","    (text, [f\"str_{i}\",]) for i, text in enumerate(data)\n","]\n","print(\"##\"*20)\n","print(\"tags and its texts\")\n","print(\"##\"*20)\n","for text, tags in common_texts_and_tags:\n","    print(f\"tags: {tags}, text: {text}\")"],"metadata":{"id":"02AfXYkq5lbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# words = 단어 list, tags = 문서ID\n","TRAIN_documents = [TaggedDocument(words=text, tags=tags) for text, tags in common_texts_and_tags]"],"metadata":{"id":"xxWU0bMH9MgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Doc2Vec(TRAIN_documents, vector_size=100, window=5, epochs=40, min_count=5, workers=4)"],"metadata":{"id":"YiBJj-q_989a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text, tags in common_texts_and_tags:\n","    trained_doc_vec = model.docvecs[tags[0]]\n","    inferred_doc_vec = model.infer_vector(text)\n","    print(f\"tags: {tags}, text: {text}\")\n","    print(f\"trained_doc_vec: {trained_doc_vec}\")\n","    print(f\"inferred_doc_vec: {inferred_doc_vec}\")\n","    print(\"--\"*20)\n","    # train set 수가 적어서, train과 infer이 조~금 다름 그래두 뭐 비슷함"],"metadata":{"id":"93ZabNXV-eOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc1 = '비싸지 않아서 좋아요. 오랫동안 촉촉하고 유분감은 느껴지지 않아요. 좋은 성분이 많이 들어있어요. ' \n","doc2 = '너무 마음에 들어요. 샘플을 줘서 좋았고 제가 여드름이 좀 나고 열감이 자주 오르는 피부인데 잘 맞았어요. 끈적이지 않고 촉촉해서 답답하지 않아서 좋아요. 흡수도 잘 되고, 바르면 겉은 보송하고 속은 촉촉해요. 복합성이나 수분 부족형에게 추천입니다.'"],"metadata":{"id":"i61mMNuJ-jQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = [doc1, doc2]"],"metadata":{"id":"OooZ3J9j-8kw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = [doc1, doc2]\n","new_documents = [\n","    ['오랫동안', '유분', '성분'], \n","    ['마음', '샘플', '제', '여드름', '열감', '피부', '흡수', '복합', '수분', '부족', '추천']\n","]\n","# predict training set with its infering vector)\n","for i, text in enumerate(new_documents): \n","    inferred_v = model.infer_vector(text)\n","    # 현재 doc를 모델을 사용하여 벡터화할때의 값 \n","    print(docs[i])\n","    print(f\"vector of {text}: {inferred_v}\")\n","    # 기학습된 문서중에서 현재 벡터와 가장 유사한 벡터를 가지는 문서를 topn만큼 추출합니다. \n","    most_similar_docs = model.docvecs.most_similar([inferred_v], topn=3)\n","    # index와 그 유사도를 함께 보여줍니다. \n","    # index(tag)가 아닌 문서를 바로 보여주기는 어려운 것 같고, \n","    for index, similarity in most_similar_docs:\n","        print(df.iloc[int(index[4:]),0])\n","        \n","        print(f\"{index}, similarity: {similarity}\")\n","    #print(most_similar_docs)\n","    print(\"==\"*20)"],"metadata":{"id":"gTq5gNOe-_-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","from sklearn import metrics\n"],"metadata":{"id":"F_lnOko-_BMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.docvecs.vectors_docs"],"metadata":{"id":"3JIrUMds3h94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k_list = list(range(2,10))\n","s_score_list = []\n","\n","for k in k_list:\n","  print('k:',k)\n","\n","  Clustering_Method = KMeans(n_clusters=k, random_state=0)\n","  X = model.docvecs.vectors_docs\n","  Clustering_Method.fit(X)\n","\n","  # 분포 확인 \n","  print(pd.DataFrame(Clustering_Method.labels_)[0].value_counts())\n","\n","  # silhouette_score\n","  s_score = metrics.silhouette_score(X, labels = Clustering_Method.labels_, metric='euclidean', random_state=0)\n","  s_score_list.append(s_score)\n","  print('silhouette_score:',s_score)\n","\n","  print('======================')\n","\n","  # 분포가 균일하지 않음. 특정 라벨만 너무 많다."],"metadata":{"id":"BRqbz7O2_DIu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(k_list, s_score_list)\n","# elbow point가 없다. "],"metadata":{"id":"BEm3yDN-_GiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Clustering_Method = KMeans(n_clusters=5, random_state=0)\n","X = model.docvecs.vectors_docs\n","Clustering_Method.fit(X)\n","\n","# 분포 확인 \n","print(pd.DataFrame(Clustering_Method.labels_)[0].value_counts())"],"metadata":{"id":"D_k3i89R_5Uu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_dict = {i:[] for i in range(0, 5)}\n","for text_tags, label in zip(common_texts_and_tags, Clustering_Method.labels_):\n","    text, tags = text_tags\n","    cluster_dict[label].append(text)\n","\n","cluster_num = []\n","for label, lst in cluster_dict.items():\n","    print(f\"Cluster {label}\")\n","    for x in lst:\n","        cluster_num.append(label)\n","        print(x)"],"metadata":{"id":"-7tbaj73_ONy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문서의 feature(단어별) cluster_centers_확인해보자\n","cluster_centers = Clustering_Method.cluster_centers_\n","print(cluster_centers.shape)\n","print(cluster_centers)\n","# shape의 행은 클러스터 레이블, 열은 벡터화 시킨 feature(단어들)"],"metadata":{"id":"XHPJx80fBuZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(cluster_num)"],"metadata":{"id":"UbGJ3TFB_lCw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_num"],"metadata":{"id":"4eHuOCAI-ymn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['cluster_label'] = cluster_num"],"metadata":{"id":"-3jwums5AXX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"FvJgS0kpA8-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Clustering_Method.cluster_centers_.argsort()[:,::-1]"],"metadata":{"id":"jeMMK2Q__is9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_cluster_details(cluster_model, cluster_data, feature_names,\n","                       cluster_num, top_n_features=10):\n","    cluster_details = {}\n","    # 각 클러스터 레이블별 feature들의 center값들 내림차순으로 정렬 후의 인덱스를 반환\n","    center_feature_idx = cluster_model.cluster_centers_.argsort()[:,::-1]\n","    \n","    # 개별 클러스터 레이블별로 \n","    for cluster_num in range(cluster_num):\n","        # 개별 클러스터별 정보를 담을 empty dict할당\n","        cluster_details[cluster_num] = {}\n","        cluster_details[cluster_num]['cluster'] = cluster_num\n","        \n","        # 각 feature별 center값들 정렬한 인덱스 중 상위 10개만 추출\n","        top_ftr_idx = center_feature_idx[cluster_num, :top_n_features]\n","        top_ftr = [feature_names[idx] for idx in top_ftr_idx]\n","        # top_ftr_idx를 활용해서 상위 10개 feature들의 center값들 반환\n","        # 반환하게 되면 array이기 떄문에 리스트로바꾸기\n","        top_ftr_val = cluster_model.cluster_centers_[cluster_num, top_ftr_idx].tolist()\n","        \n","        # cluster_details 딕셔너리에다가 개별 군집 정보 넣어주기\n","        cluster_details[cluster_num]['top_features'] = top_ftr\n","        cluster_details[cluster_num]['top_featrues_value'] = top_ftr_val\n","        # 해당 cluster_num으로 분류된 파일명(문서들) 넣어주기\n","        filenames = cluster_data[cluster_data['cluster_label']==cluster_num]['리뷰']\n","        # filenames가 df으로 반환되기 떄문에 값들만 출력해서 array->list로 변환\n","        filenames = filenames.values.tolist()\n","        cluster_details[cluster_num]['filenames'] = filenames\n","    \n","    return cluster_details\n","\n","def print_cluster_details(cluster_details):\n","    for cluster_num, cluster_detail in cluster_details.items():\n","        print(f\"#####Cluster Num: {cluster_num}\")\n","        print()\n","        print(\"상위 10개 feature단어들:\\n\", cluster_detail['top_features'])\n","        print()\n","        print(f\"Cluster {cluster_num}으로 분류된 문서들:\\n{cluster_detail['filenames'][:5]}\")\n","        print('-'*20)\n","\n","#feature_names = tfidf_vect.get_feature_names()\n","cluster_details = get_cluster_details(cluster_model=Clustering_Method,\n","                                     cluster_data=df,\n","                                     feature_names=my_list,\n","                                     cluster_num=5,\n","                                     top_n_features=10)\n","print_cluster_details(cluster_details)"],"metadata":{"id":"wiD7tHVK44y-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model."],"metadata":{"id":"Su-yWfaP5z2g"},"execution_count":null,"outputs":[]}]}